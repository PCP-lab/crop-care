{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8318b9",
   "metadata": {},
   "source": [
    "\n",
    "# DeBERTa v3 — Phase 3 Fine-tune (CropCare)\n",
    "\n",
    "This notebook fine-tunes a text classifier on your farmer-style descriptions using the datasets you copied into:\n",
    "\n",
    "```\n",
    "/home/myid/bp67339/plant_disease/data/\n",
    "  - train_clean_step1.jsonl\n",
    "  - train_phase3_augmented.jsonl\n",
    "  - train_phase3_pairs.jsonl\n",
    "  - val_clean_step1.jsonl   (optional; if missing we do a 90/10 split)\n",
    "```\n",
    "\n",
    "**Outputs** are saved to:\n",
    "\n",
    "```\n",
    "/home/myid/bp67339/plant_disease/models/deberta_v3_base_textclf_phase3/\n",
    "```\n",
    "and include `model.safetensors`, tokenizer files, and `labels.json` so your FastAPI app can load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5021d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myid/bp67339/trainenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.57.1\n",
      "accelerate  : 1.0.1\n",
      "accelerate file: /home/myid/bp67339/trainenv/lib/python3.10/site-packages/accelerate/__init__.py\n",
      "unwrap_model signature: (self, model, keep_fp32_wrapper: 'bool' = True)\n",
      "sys.path[0:5]: ['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/myid/bp67339/trainenv/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, inspect\n",
    "import accelerate, transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"accelerate  :\", accelerate.__version__)\n",
    "print(\"accelerate file:\", accelerate.__file__)\n",
    "print(\"unwrap_model signature:\", inspect.signature(Accelerator.unwrap_model))\n",
    "print(\"sys.path[0:5]:\", sys.path[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d65c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.0.1\n",
      "(self, model, keep_fp32_wrapper: 'bool' = True)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from accelerate import Accelerator\n",
    "print(\"accelerate:\", __import__(\"accelerate\").__version__)\n",
    "print(inspect.signature(Accelerator.unwrap_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362c8773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True NVIDIA GeForce RTX 3090\n",
      "torch: 2.2.2+cu118\n",
      "numpy: 1.26.4 pandas: 2.2.2\n",
      "transformers: 4.57.1\n",
      "datasets: 2.19.1\n",
      "accelerate: 1.0.1\n",
      "evaluate: 0.4.2\n",
      "sklearn: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, pandas as pd\n",
    "import transformers, datasets, accelerate, evaluate, sklearn\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"numpy:\", np.__version__, \"pandas:\", pd.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"evaluate:\", evaluate.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84912f48",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports, paths, and config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a5ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os, json, random, numpy as np, torch\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Paths (anchored to your plant_disease folder)\n",
    "HOME     = Path(\"/home/myid/bp67339\")\n",
    "ROOT     = HOME / \"plant_disease\"\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"models\" / \"deberta_v3_base_textclf_phase3\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prefer local checkpoints; fall back to HF base if not present\n",
    "LOCAL_BASES = [\n",
    "    ROOT / \"models\" / \"deberta_v3_base_textclf_phase2\",\n",
    "    ROOT / \"models\" / \"deberta_v3_base_textclf\",\n",
    "]\n",
    "HF_BASE = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# Training data files (use whichever exist)\n",
    "TRAIN_FILES = [\n",
    "    DATA_DIR / \"train_clean_step1.jsonl\",\n",
    "    DATA_DIR / \"train_phase3_augmented.jsonl\",\n",
    "    DATA_DIR / \"train_phase3_pairs.jsonl\",\n",
    "]\n",
    "VAL_FILE = DATA_DIR / \"val_clean_step1.jsonl\"\n",
    "\n",
    "# Hyperparams\n",
    "MAX_LEN   = 256\n",
    "SEED      = 42\n",
    "LR        = 2e-5\n",
    "EPOCHS    = 3\n",
    "BATCH_T   = 16\n",
    "BATCH_E   = 32\n",
    "LOG_STEPS = 50\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959e976",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Seed and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae033781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_jsonl(paths):\n",
    "    ps = [str(p) for p in paths if Path(p).is_file()]\n",
    "    if not ps: \n",
    "        return None\n",
    "    return load_dataset(\"json\", data_files={\"data\": ps})[\"data\"]\n",
    "\n",
    "def pick_base_checkpoint():\n",
    "    for p in LOCAL_BASES:\n",
    "        if p.is_dir():\n",
    "            return str(p), True  # local_only\n",
    "    return HF_BASE, False\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec53107",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Load datasets\n",
    "Each JSONL row should be like:\n",
    "```json\n",
    "{\"text\": \"brown circular spots...\", \"label\": \"Cercospora Leaf Spot\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c441440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] using: /home/myid/bp67339/plant_disease/data/train_clean_step1.jsonl\n",
      "[data] using: /home/myid/bp67339/plant_disease/data/train_phase3_augmented.jsonl\n",
      "[data] using: /home/myid/bp67339/plant_disease/data/train_phase3_pairs.jsonl\n",
      "Sizes: 140843 7675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_parts = []\n",
    "for fp in TRAIN_FILES:\n",
    "    ds = load_jsonl([fp])\n",
    "    if ds is not None:\n",
    "        train_parts.append(ds)\n",
    "        print(f\"[data] using: {fp}\")\n",
    "    else:\n",
    "        print(f\"[data] missing: {fp}\")\n",
    "\n",
    "if not train_parts:\n",
    "    raise SystemExit(\"No training files found under plant_disease/data/\")\n",
    "\n",
    "ds_train = concatenate_datasets(train_parts) if len(train_parts) > 1 else train_parts[0]\n",
    "ds_eval  = load_jsonl([VAL_FILE])\n",
    "\n",
    "if ds_eval is None:\n",
    "    print(\"[data] no explicit val file; creating 10% split from train\")\n",
    "    split = ds_train.train_test_split(test_size=0.10, seed=SEED)\n",
    "    ds_train, ds_eval = split[\"train\"], split[\"test\"]\n",
    "\n",
    "for name, ds in [(\"train\", ds_train), (\"eval\", ds_eval)]:\n",
    "    assert \"text\" in ds.column_names and \"label\" in ds.column_names, f\"{name} needs 'text' and 'label'\"\n",
    "print(\"Sizes:\", len(ds_train), len(ds_eval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86814ed5",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Labels and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a8fe176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model] base: /home/myid/bp67339/plant_disease/models/deberta_v3_base_textclf_phase2 (local_only=True)\n",
      "✅ tokenized fields: ['label', 'text_norm', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "sizes: 140843 7675 num_labels: 6\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize text into a single column ---------------------------------------\n",
    "import math\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def _as_str(x):\n",
    "    if x is None: return \"\"\n",
    "    if isinstance(x, float) and math.isnan(x): return \"\"\n",
    "    if isinstance(x, (int, float, bool)): return str(x)\n",
    "    if isinstance(x, list): return \" \".join(str(t) for t in x if t is not None)\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"text\", \"variant\", \"orig\", \"clean\", \"message\", \"description\"):\n",
    "            if k in x and x[k] not in (None, float(\"nan\")):\n",
    "                return _as_str(x[k])\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "def normalize_text_row(ex):\n",
    "    v = ex.get(\"text\") or ex.get(\"variant\") or ex.get(\"orig\")\n",
    "    s = _as_str(v)\n",
    "    s = \" \".join(s.split())\n",
    "    return {\"text_norm\": s}\n",
    "\n",
    "ds_train = ds_train.map(normalize_text_row, desc=\"normalize train text\")\n",
    "ds_eval  = ds_eval.map(normalize_text_row,  desc=\"normalize eval text\")\n",
    "\n",
    "# Drop empties just in case\n",
    "ds_train = ds_train.filter(lambda ex: len(ex[\"text_norm\"]) > 0, desc=\"drop empty train\")\n",
    "ds_eval  = ds_eval.filter(lambda ex: len(ex[\"text_norm\"])  > 0, desc=\"drop empty eval\")\n",
    "\n",
    "# --- Build stable class list from TRAIN ONLY -----------------------------------\n",
    "labels   = sorted(set(ds_train[\"label\"]))\n",
    "label2id = {n: i for i, n in enumerate(labels)}\n",
    "id2label = {i: n for n, i in label2id.items()}\n",
    "\n",
    "def map_labels(ex):\n",
    "    return {\"label\": label2id[ex[\"label\"]]}\n",
    "\n",
    "ds_train = ds_train.map(map_labels, desc=\"map train labels\")\n",
    "\n",
    "# keep only eval rows whose labels exist in train (guard against drift)\n",
    "ds_eval  = ds_eval.filter(lambda ex: ex[\"label\"] in label2id, desc=\"keep eval known labels\")\n",
    "ds_eval  = ds_eval.map(map_labels, desc=\"map eval labels\")\n",
    "\n",
    "# --- Tokenizer -----------------------------------------------------------------\n",
    "base_ckpt, local_only = pick_base_checkpoint()\n",
    "print(f\"[model] base: {base_ckpt} (local_only={local_only})\")\n",
    "tok = AutoTokenizer.from_pretrained(base_ckpt, use_fast=False, local_files_only=local_only)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tok(batch[\"text_norm\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "# remove everything except label + text_norm (tokenizer will replace text_norm with tensors)\n",
    "cols_tr = [c for c in ds_train.column_names if c not in (\"label\", \"text_norm\")]\n",
    "cols_ev = [c for c in ds_eval.column_names  if c not in (\"label\", \"text_norm\")]\n",
    "\n",
    "ds_train_tok = ds_train.map(tok_fn, batched=True, remove_columns=cols_tr, desc=\"tokenize train\")\n",
    "ds_eval_tok  = ds_eval.map(tok_fn,  batched=True, remove_columns=cols_ev,  desc=\"tokenize eval\")\n",
    "\n",
    "# --- Sanity checks -------------------------------------------------------------\n",
    "assert \"input_ids\" in ds_train_tok.features, \"tokenization failed (train)\"\n",
    "assert \"input_ids\" in ds_eval_tok.features,  \"tokenization failed (eval)\"\n",
    "print(\"✅ tokenized fields:\", list(ds_train_tok.features.keys()))\n",
    "print(\"sizes:\", len(ds_train_tok), len(ds_eval_tok), \"num_labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05c359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate: 1.0.1\n",
      "unwrap_model signature: (self, model, keep_fp32_wrapper: 'bool' = True)\n",
      "✅ Patched accelerate.Accelerator.unwrap_model to ignore extra kwargs.\n"
     ]
    }
   ],
   "source": [
    "import inspect, types\n",
    "from accelerate import Accelerator\n",
    "import accelerate\n",
    "\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"unwrap_model signature:\", inspect.signature(Accelerator.unwrap_model))\n",
    "\n",
    "# Patch only if unwrap_model doesn't accept **kwargs\n",
    "sig = inspect.signature(Accelerator.unwrap_model)\n",
    "has_var_kw = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())\n",
    "if not has_var_kw:\n",
    "    _orig_unwrap = Accelerator.unwrap_model\n",
    "    def _unwrap_compat(self, model, *args, **kwargs):\n",
    "        # Ignore unknown kwargs e.g. keep_torch_compile from transformers\n",
    "        return _orig_unwrap(self, model, *args)\n",
    "    Accelerator.unwrap_model = types.MethodType(_unwrap_compat, Accelerator)\n",
    "    print(\"✅ Patched accelerate.Accelerator.unwrap_model to ignore extra kwargs.\")\n",
    "else:\n",
    "    print(\"✅ unwrap_model already accepts **kwargs; no patch needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78589247",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model, Trainer, and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f58c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/tmp/ipykernel_1485996/952763177.py:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/myid/bp67339/trainenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13206' max='13206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13206/13206 1:15:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.151142</td>\n",
       "      <td>0.973420</td>\n",
       "      <td>0.971455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.151098</td>\n",
       "      <td>0.978893</td>\n",
       "      <td>0.977170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115406</td>\n",
       "      <td>0.984235</td>\n",
       "      <td>0.982919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myid/bp67339/trainenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/myid/bp67339/trainenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13206, training_loss=0.020729483354480438, metrics={'train_runtime': 4510.5979, 'train_samples_per_second': 93.675, 'train_steps_per_second': 2.928, 'total_flos': 2.341359158497128e+16, 'train_loss': 0.020729483354480438, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "# ---- model --------------------------------------------------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_ckpt,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    torch_dtype=\"auto\",\n",
    "    local_files_only=local_only,\n",
    ").to(device)\n",
    "\n",
    "# ---- metrics ------------------------------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    f1m = f1_score(p.label_ids, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "collate = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "# ---- build args dict compatibly -----------------------------------------\n",
    "# Some TF versions use \"evaluation_strategy\", some \"eval_strategy\".\n",
    "candidate_args = {\n",
    "    \"output_dir\": str(OUT_DIR),\n",
    "    \"learning_rate\": LR,\n",
    "    \"per_device_train_batch_size\": BATCH_T,\n",
    "    \"per_device_eval_batch_size\": BATCH_E,\n",
    "    \"num_train_epochs\": EPOCHS,\n",
    "    # both names; we'll keep whichever your version supports\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"f1_macro\",\n",
    "    \"logging_steps\": LOG_STEPS,\n",
    "    # mixed precision (prefer bf16 on Ampere+; else fp16 if CUDA; else off)\n",
    "    \"bf16\": (torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8),\n",
    "    \"fp16\": (torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8),\n",
    "    \"report_to\": [],                 # disable wandb/etc\n",
    "    \"save_safetensors\": True,\n",
    "}\n",
    "\n",
    "# keep only fields TrainingArguments actually supports\n",
    "field_names = set(getattr(TrainingArguments, \"__dataclass_fields__\", {}).keys())\n",
    "filtered_args = {k: v for k, v in candidate_args.items() if k in field_names}\n",
    "\n",
    "# if neither eval field existed, you’ll just do no periodic eval (fine)\n",
    "args = TrainingArguments(**filtered_args)\n",
    "\n",
    "# ---- trainer ------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collate,\n",
    "    train_dataset=ds_train_tok,\n",
    "    eval_dataset=ds_eval_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfeed60",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Save artifacts (model, tokenizer, labels.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3851e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/myid/bp67339/plant_disease/models/deberta_v3_base_textclf_phase3\n",
      "['checkpoint-8804', 'tokenizer_config.json', 'labels.json', 'model.safetensors', 'checkpoint-4402', 'config.json', 'checkpoint-13206', 'spm.model', 'special_tokens_map.json', 'added_tokens.json', 'training_args.bin']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "trainer.save_model(OUT_DIR)\n",
    "\n",
    "with open(OUT_DIR / \"labels.json\", \"w\") as f:\n",
    "    json.dump({\"labels\": labels}, f, indent=2)\n",
    "\n",
    "print(\"Saved to:\", OUT_DIR)\n",
    "print([p.name for p in OUT_DIR.iterdir()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae08d37",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Sanity check: reload and one example inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08757747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: Cercospora Leaf Spot p= 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tok2 = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=False, local_files_only=True)\n",
    "mdl2 = AutoModelForSequenceClassification.from_pretrained(OUT_DIR, local_files_only=True).to(device).eval()\n",
    "\n",
    "sample = \"brown circular spots with yellow halo\"\n",
    "enc = tok2(sample, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = mdl2(**enc).logits\n",
    "probs = F.softmax(logits, dim=-1).squeeze(0).tolist()\n",
    "\n",
    "top_i = int(np.argmax(probs))\n",
    "print(\"pred:\", labels[top_i], \"p=\", round(probs[top_i], 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
